# System Architecture - VIVID Project

## ğŸ—ï¸ High-Level Architecture

VIVID follows a **multi-pipeline sensor fusion architecture** that combines computer vision, inertial sensors, and intelligent decision-making for real-time assistive navigation.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input Layer   â”‚    â”‚ Processing Layer â”‚    â”‚  Output Layer   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Camera Feed   â”‚â”€â”€â”€â–¶â”‚ â€¢ Object Detect. â”‚â”€â”€â”€â–¶â”‚ â€¢ Audio Guide   â”‚
â”‚ â€¢ IMU Sensors   â”‚    â”‚ â€¢ Depth Estimate â”‚    â”‚ â€¢ Visual Feed   â”‚
â”‚ â€¢ User Motion   â”‚    â”‚ â€¢ Motion Track   â”‚    â”‚ â€¢ Risk Alerts   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â€¢ Sensor Fusion  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Three-Pipeline Design

### Pipeline 1: Visual Processing
```
Camera Frame â†’ YOLO v8 â†’ Object Detection â†’ Bounding Boxes
             â†“
          MiDaS â†’ Depth Estimation â†’ Distance Map
             â†“
        DeepSORT â†’ Multi-Object Tracking â†’ Persistent IDs
```

### Pipeline 2: Motion Analysis  
```
IMU Data â†’ Accelerometer/Gyroscope â†’ Motion State
        â†“
Optical Flow â†’ Camera Motion â†’ Ego-Motion Compensation
        â†“
Sensor Fusion â†’ Combined Motion Estimate
```

### Pipeline 3: Decision Making
```
Visual Tracks + Motion Data â†’ Risk Assessment â†’ Priority Queue
                           â†“
              Spatial Reasoning â†’ 3D Audio Guidance
                           â†“
              Speech Synthesis â†’ User Feedback
```

## ğŸ§© Component Architecture

### Core Components

```
EnhancedObstacleDetector (Main Controller)
â”œâ”€â”€ YOLOv8Detector
â”‚   â”œâ”€â”€ Model Loading
â”‚   â”œâ”€â”€ Object Detection
â”‚   â””â”€â”€ Confidence Filtering
â”œâ”€â”€ DepthEstimator (MiDaS)
â”‚   â”œâ”€â”€ Monocular Depth
â”‚   â”œâ”€â”€ Depth Map Generation
â”‚   â””â”€â”€ Distance Calculation
â”œâ”€â”€ DeepSORTTracker
â”‚   â”œâ”€â”€ Feature Extraction
â”‚   â”œâ”€â”€ Kalman Filtering
â”‚   â””â”€â”€ Hungarian Assignment
â”œâ”€â”€ IMUProcessor
â”‚   â”œâ”€â”€ Sensor Data Buffer
â”‚   â”œâ”€â”€ Motion Estimation
â”‚   â””â”€â”€ Coordinate Transform
â”œâ”€â”€ DataFusion
â”‚   â”œâ”€â”€ Multi-Sensor Integration
â”‚   â”œâ”€â”€ Risk Assessment
â”‚   â””â”€â”€ Decision Logic
â””â”€â”€ GuidanceSystem
    â”œâ”€â”€ Spatial Audio
    â”œâ”€â”€ Text-to-Speech
    â””â”€â”€ Priority Management
```

## ğŸ“Š Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Camera (30  â”‚â”€â”€â”
â”‚ FPS)        â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phone IMU   â”‚â”€â–¶â”‚   Frame Sync    â”‚
â”‚ (100 Hz)    â”‚  â”‚   & Buffer      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Object Detectionâ”‚
                 â”‚ (YOLO v8)       â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Depth Estimationâ”‚    â”‚ Object Tracking â”‚
                 â”‚ (MiDaS)         â”‚    â”‚ (DeepSORT)      â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚                       â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  Data Fusion    â”‚
                         â”‚  & Risk Assess  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚ Guidance System â”‚
                         â”‚ (Audio Output)  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Processing Pipelines Detail

### 1. Computer Vision Pipeline

**Input:** BGR Video Frame (640x480 @ 30 FPS)

**Processing Steps:**
1. **Frame Preprocessing**
   ```python
   # Resize for performance
   if resize_factor < 1.0:
       frame = cv2.resize(frame, None, fx=factor, fy=factor)
   ```

2. **Object Detection (YOLO v8)**
   ```python
   # Every 3rd frame (performance optimization)
   if frame_count % (skip_frames + 1) == 0:
       results = model(frame, conf=0.5, iou=0.6)
   ```

3. **Depth Estimation (MiDaS)**
   ```python
   # Every 6th frame (heavy computation)
   if frame_count % (depth_skip_frames + 1) == 0:
       depth_map = midas_model(input_tensor)
   ```

4. **Multi-Object Tracking (DeepSORT)**
   ```python
   # Every frame (maintain continuity)
   tracks = tracker.update(detections, frame)
   ```

**Output:** List of tracked objects with 3D positions

### 2. Sensor Fusion Pipeline

**Input:** IMU data stream (100+ Hz) + Visual tracks (30 Hz)

**Processing Steps:**
1. **IMU Data Buffering**
   ```python
   # Circular buffer for sensor history
   imu_history = deque(maxlen=100)
   ```

2. **Motion State Estimation**
   ```python
   # Integrate acceleration for velocity
   # Apply coordinate transformations
   motion_state = estimate_motion(imu_data_list)
   ```

3. **Ego-Motion Compensation**
   ```python
   # Distinguish user motion from object motion
   compensated_velocity = object_velocity - user_velocity
   ```

**Output:** Motion-compensated object velocities

### 3. Decision Making Pipeline

**Input:** Tracked objects + Motion data + User context

**Processing Steps:**
1. **Risk Assessment**
   ```python
   risk_score = calculate_risk(distance, velocity, object_class)
   ```

2. **Spatial Prioritization**
   ```python
   # Prioritize by distance and trajectory
   priority_queue = sort_by_collision_risk(objects)
   ```

3. **Guidance Generation**
   ```python
   # Generate contextual audio guidance
   guidance_text = generate_spatial_guidance(priority_objects)
   ```

**Output:** Audio guidance commands

## ğŸ”§ Threading Architecture

### Multi-Threading Design

```
Main Thread (GUI)
â”œâ”€â”€ VideoThread
â”‚   â”œâ”€â”€ Frame Capture (30 FPS)
â”‚   â”œâ”€â”€ Processing Pipeline
â”‚   â””â”€â”€ Display Updates
â”œâ”€â”€ NetworkThread
â”‚   â”œâ”€â”€ TCP Server (IMU Commands)
â”‚   â”œâ”€â”€ UDP Server (Sensor Data)
â”‚   â””â”€â”€ Data Buffering
â”œâ”€â”€ AudioThread
â”‚   â”œâ”€â”€ TTS Queue Processing
â”‚   â”œâ”€â”€ Speech Synthesis
â”‚   â””â”€â”€ Audio Output
â””â”€â”€ ProcessingThread
    â”œâ”€â”€ Heavy Computations
    â”œâ”€â”€ Model Inference
    â””â”€â”€ Background Tasks
```

### Thread Communication

```python
# Thread-safe communication using Qt signals
class VideoThread(QThread):
    change_pixmap_signal = pyqtSignal(np.ndarray)
    detection_signal = pyqtSignal(list)
    
    def run(self):
        # Processing loop
        processed_frame, detections = self.detector.process_frame(frame)
        
        # Emit signals to main thread
        self.change_pixmap_signal.emit(processed_frame)
        self.detection_signal.emit(detections)
```

## ğŸ“¡ Network Architecture

### Client-Server Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Smartphone    â”‚  WiFi   â”‚   Computer      â”‚
â”‚   (Client)      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   (Server)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Sensor Data   â”‚  UDP    â”‚ â€¢ Processing    â”‚
â”‚ â€¢ IMU Stream    â”‚ 8889    â”‚ â€¢ AI Models     â”‚
â”‚ â€¢ JSON Format   â”‚         â”‚ â€¢ GUI Display   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  TCP    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     8888
```

### Communication Protocols

**UDP (Sensor Data):**
- High frequency (100+ Hz)
- Low latency
- Packet loss acceptable
- JSON format

**TCP (Commands):**
- Reliable delivery
- Configuration changes
- Status updates
- Error handling

## ğŸ§  AI Model Architecture

### YOLO v8 Integration

```python
# Model loading and configuration
model = YOLO("yolov8n.pt").to(device)
model.conf = 0.5  # Confidence threshold
model.iou = 0.6   # NMS threshold
```

**Processing Flow:**
1. Input preprocessing (640x640 normalization)
2. Neural network inference
3. Non-maximum suppression
4. Coordinate transformation
5. Class filtering (80 COCO classes)

### MiDaS Depth Estimation

```python
# Transform pipeline
transform = torch.hub.load("intel-isl/MiDaS", "transforms").dpt_transform

# Inference process
input_tensor = transform(rgb_image).unsqueeze(0)
with torch.no_grad():
    depth_map = model(input_tensor)
```

**Depth Processing:**
1. RGB â†’ Normalized tensor
2. DPT (Dense Prediction Transformer)
3. Relative depth map
4. Bicubic interpolation
5. Distance estimation

### DeepSORT Tracking

```python
# Feature extraction + tracking
features = feature_extractor(object_crops)
tracks = kalman_filter.update(detections, features)
```

**Tracking Pipeline:**
1. Feature extraction (CNN)
2. Appearance matching
3. Kalman filter prediction
4. Hungarian assignment
5. Track management

## ğŸ”„ State Management

### Application State

```python
@dataclass
class SystemState:
    # Processing state
    is_running: bool = False
    is_paused: bool = False
    current_frame: Optional[np.ndarray] = None
    
    # Detection state
    active_tracks: Dict[int, TrackInfo] = field(default_factory=dict)
    last_detections: List[Dict] = field(default_factory=list)
    
    # Sensor state
    imu_connected: bool = False
    last_imu_data: Optional[IMUData] = None
    motion_state: Optional[MotionState] = None
    
    # Configuration state
    voice_enabled: bool = True
    detection_threshold: float = 0.5
    processing_resolution: Tuple[int, int] = (640, 480)
```

### Track Management

```python
# Persistent object tracking across frames
track_states = {
    track_id: {
        'first_seen': timestamp,
        'last_seen': timestamp,
        'position_history': deque(maxlen=30),
        'velocity_history': deque(maxlen=10),
        'confidence_history': deque(maxlen=5)
    }
}
```

## âš¡ Performance Optimizations

### Computational Optimizations

1. **Frame Skipping Strategy**
   ```python
   # Different skip rates for different models
   YOLO_SKIP = 2      # Every 3rd frame
   MIDAS_SKIP = 5     # Every 6th frame
   TRACKING_SKIP = 0  # Every frame
   ```

2. **Resolution Scaling**
   ```python
   # Reduce resolution for speed
   RESIZE_FACTOR = 0.5  # Process at 50% resolution
   ```

3. **GPU Memory Management**
   ```python
   # Clear GPU cache periodically
   if frame_count % 100 == 0:
       torch.cuda.empty_cache()
   ```

### Memory Optimizations

```python
# Circular buffers for sensor data
imu_buffer = deque(maxlen=100)    # ~1 second of IMU data
depth_cache = deque(maxlen=10)    # Cache depth maps
track_history = deque(maxlen=30)  # 1 second of tracking
```

## ğŸ” Error Handling Architecture

### Exception Hierarchy

```
VividException (Base)
â”œâ”€â”€ CameraError
â”‚   â”œâ”€â”€ CameraNotFoundError
â”‚   â”œâ”€â”€ CameraPermissionError
â”‚   â””â”€â”€ CameraTimeoutError
â”œâ”€â”€ ModelError
â”‚   â”œâ”€â”€ ModelLoadError
â”‚   â”œâ”€â”€ InferenceError
â”‚   â””â”€â”€ CudaOutOfMemoryError
â”œâ”€â”€ NetworkError
â”‚   â”œâ”€â”€ ServerStartError
â”‚   â”œâ”€â”€ ConnectionTimeoutError
â”‚   â””â”€â”€ DataFormatError
â””â”€â”€ ProcessingError
    â”œâ”€â”€ FrameProcessingError
    â”œâ”€â”€ TrackingError
    â””â”€â”€ SensorFusionError
```

### Recovery Strategies

```python
# Graceful degradation approach
try:
    # Full pipeline with all features
    result = process_with_all_features(frame)
except CudaOutOfMemoryError:
    # Fallback to CPU processing
    result = process_with_cpu_only(frame)
except ModelLoadError:
    # Use basic OpenCV detection
    result = process_with_fallback_detection(frame)
```

## ğŸ“ˆ Scalability Design

### Modular Architecture

```
Core Module (vivid.py)
â”œâ”€â”€ Detection Module
â”œâ”€â”€ Tracking Module  
â”œâ”€â”€ Depth Module
â”œâ”€â”€ IMU Module
â”œâ”€â”€ Fusion Module
â””â”€â”€ Guidance Module
```

Each module can be:
- **Independently updated**
- **Swapped with alternatives**
- **Configured separately**
- **Tested in isolation**

### Extension Points

```python
# Plugin architecture for new models
class DetectorPlugin:
    def detect(self, frame: np.ndarray) -> List[Detection]:
        pass

# Register new detectors
detector_registry = {
    'yolo8': YOLO8Detector,
    'yolo11': YOLO11Detector,  # Future version
    'custom': CustomDetector
}
```

This architecture ensures VIVID remains maintainable, extensible, and performant while handling the complexity of real-time multi-sensor fusion.
